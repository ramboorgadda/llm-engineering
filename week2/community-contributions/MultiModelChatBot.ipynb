{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de436436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown,display,update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47de968c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai api key starts with sk-proj-\n",
      "anthropic api key starts with sk-a\n",
      "google api key starts with AIzaSyDC\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"openai api key starts with {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(f\"openai api key is not found\")\n",
    "if anthropic_api_key:\n",
    "    print(f\"anthropic api key starts with {anthropic_api_key[:4]}\")\n",
    "else:\n",
    "    print(f\"anthropic api key is not found\")\n",
    "if google_api_key:\n",
    "    print(f\"google api key starts with {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(f\"google api key is not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Anthropic' object has no attribute 'Anthropic'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m openai = OpenAI()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m claude = \u001b[43manthropic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAnthropic\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'Anthropic' object has no attribute 'Anthropic'"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e432ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationManager:\n",
    "    def __init__(self):\n",
    "        self.conversation_history=[]\n",
    "        self.participants={}\n",
    "    \n",
    "    def add_participant(self,name,chatbot):\n",
    "        \"\"\"Adding Model to the Participant\"\"\"\n",
    "        self.participants[name]=chatbot\n",
    "    \n",
    "    def add_message(self,speaker,message):\n",
    "        \"\"\"Add message to conversation history\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"role\": \"assistant\" if speaker in self.participants else \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "    def get_context_for_model(self,model_name):\n",
    "        \"\"\"Convert the shared history to model specific format\"\"\"\n",
    "        messages =[]\n",
    "        for msg in self.conversation_history:\n",
    "            if msg[\"speaker\"] == model_name:\n",
    "                messages.append({\"role\":\"assistant\",\"content\":msg[\"content\"]})\n",
    "            else:\n",
    "                messages.append({\"role\":\"user\",\"content\":msg[\"content\"]})\n",
    "        return messages\n",
    "    def run_conversation(self,starting_message,turns=3,round_robin=True):\n",
    "        \"\"\"run a multi-model conversation for specific number of turns\"\"\"\n",
    "        current_message = starting_message\n",
    "        models = list(self.participants.keys())\n",
    "\n",
    "        # add initial user message so models have context\n",
    "        self.add_message(\"user\", starting_message)\n",
    "\n",
    "        for _ in range(turns):\n",
    "            for model_name in models:\n",
    "                model_context = self.get_context_for_model(model_name)\n",
    "                chatbot=self.participants[model_name]\n",
    "                response=chatbot.generate_response(model_context)\n",
    "                self.add_message(model_name,response)\n",
    "                print(f\"{model_name}:\\n{response}\\n\")\n",
    "                \n",
    "                if not round_robin:\n",
    "                    # If not round-robin, use this response as input to next model\n",
    "                    current_message = response\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self,model_name,system_prompt,**kwargs):\n",
    "        self.model_name = model_name\n",
    "        self.system_prompt = system_prompt\n",
    "        self.api_key = kwargs.get('api_key', None)\n",
    "        self.base_url = kwargs.get('base_url', None)\n",
    "\n",
    "    def generate_response(self,messages):\n",
    "        \"\"\"generate a response based on the messages and don't provide history\"\"\"\n",
    "        full_messages = [{\"role\":\"system\",\"content\":self.system_prompt}] + messages\n",
    "        try:\n",
    "            if 'claude' in self.model_name.lower():\n",
    "                # use the shared claude_client created earlier\n",
    "                claude_messages = [m for m in messages if m['role']!='system']\n",
    "                response = claude.messages.create(\n",
    "                    model=self.model_name,\n",
    "                    system=self.system_prompt,\n",
    "                    messages=claude_messages,\n",
    "                    max_tokens=200\n",
    "                )\n",
    "                return response.content[0].text\n",
    "            else:\n",
    "                # use OpenAI-style chat completions\n",
    "                client = OpenAI(api_key=self.api_key, base_url=self.base_url) if (self.api_key or self.base_url) else openai\n",
    "                response = client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=full_messages\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "    \n",
    "        except Exception as e:\n",
    "            return f\"Error Str{e}\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e79d068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Oh, you mean aside from cat videos taking over the internet again? Alright, if I have to pick a “most interesting” tech trend right now, I’d say it’s probably generative AI. It’s like the tech version of your friend who never stops talking—but instead of annoying you at parties, it can write poetry, code, and even create art. It’s both amazing and mildly terrifying, kind of like a digital Frankenstein that’s learned to do your job... and maybe your therapist’s, too.\n",
      "\n",
      "Claude:\n",
      "Error Strname 'claude_client' is not defined\n",
      "\n",
      "GPT:\n",
      "Ah, the classic \"Strname 'claude_client' is not defined\" error—sounds like your code just ghosted you. This usually means you tried to use `claude_client` without actually introducing it first, kind of like inviting someone to a party and then forgetting to tell them where it is.\n",
      "\n",
      "Here’s the usual recipe to fix that:\n",
      "\n",
      "1. Make sure you’ve imported or defined `claude_client` before using it.\n",
      "2. If `claude_client` is part of a library, double-check that you installed the package and imported it correctly, like:\n",
      "   ```python\n",
      "   from your_library import claude_client\n",
      "   ```\n",
      "3. If you’re supposed to instantiate it, don’t forget the constructor:\n",
      "   ```python\n",
      "   client = claude_client()\n",
      "   ```\n",
      "\n",
      "Without a bit more context, it’s hard to be more precise. But in code terms: define your variables and imports before you use them. Your computer isn’t a mind reader... yet.\n",
      "\n",
      "Claude:\n",
      "Error Strname 'claude_client' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_bot = ChatBot(\"gpt-4.1-mini\", \"You are witty and sarcastic.\")\n",
    "claude_bot = ChatBot(\"claude-3-5-haiku-20241022\", \"You are thoughtful and philosophical.\")\n",
    "\n",
    "# Optional: local Ollama model (only if installed). Comment out if not available.\n",
    "# model_name = \"llama3.2:latest\"\n",
    "# system_prompt = \"You are a helpful assistant that is very argumentative in a snarky way.\"\n",
    "# kwargs = {\n",
    "#     \"api_key\": \"ollama\",\n",
    "#     \"base_url\": 'http://localhost:11434/v1'\n",
    "# }\n",
    "# qwen = ChatBot(model_name, system_prompt, **kwargs)\n",
    "\n",
    "# Set up conversation manager\n",
    "conversation = ConversationManager()\n",
    "conversation.add_participant(\"GPT\", gpt_bot)\n",
    "conversation.add_participant(\"Claude\", claude_bot)\n",
    "# conversation.add_participant(\"Qwen\", qwen)  # enable only if the model exists\n",
    "\n",
    "conversation.run_conversation(\"What's the most interesting technology trend right now?\", turns=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
